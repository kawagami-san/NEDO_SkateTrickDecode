{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP14wLjUEFyzekhCq7IphKA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#0.初期設定"],"metadata":{"id":"J4o6_noBS8F9"}},{"cell_type":"code","source":["#*******今回の実行の設定*******\n","\n","#プロジェクトのフォルダ　※BASE_DIRを、自らの環境に合わせて修正してください\n","BASE_DIR = '/content/drive/MyDrive/project01'\n","\n","#モデル名\n","MODEL_NAME = \"data1_trial1\"\n","\n","#動作モード：1:データ①、2:データ②、3:データ①コンペ時設定\n","ENV_MODE = 1\n","\n","#乱数シード\n","RANDOM_SEED = 42"],"metadata":{"id":"ZoLw6uJPqYjg","executionInfo":{"status":"ok","timestamp":1730643286152,"user_tz":-540,"elapsed":770,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0QWb5IfSkLu","executionInfo":{"status":"ok","timestamp":1730643845905,"user_tz":-540,"elapsed":7184,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}},"outputId":"508d0411-1087-4630-e059-f83ba7f4024f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: sktime in /usr/local/lib/python3.10/dist-packages (0.34.0)\n","Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.4.2)\n","Requirement already satisfied: numpy<2.2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.1)\n","Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (2.2.2)\n","Requirement already satisfied: scikit-base<0.12.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (0.11.0)\n","Requirement already satisfied: scikit-learn<1.6.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.5.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6.0,>=0.24->sktime) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.16.0)\n"]}],"source":["#*******Googleドライブへの接続*******\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#*******不足ライブラリのインストール*******\n","!pip install sktime\n","\n","#*******ライブラリのインポート*******\n","import sys\n","import os\n","import random\n","import pandas as pd\n","import numpy as np\n","import glob\n","import torch\n","from scipy.io import loadmat\n","%matplotlib inline\n","\n","#*******乱数シードを固定*******\n","seed = RANDOM_SEED\n","torch.manual_seed(seed)  # PyTorchのシードを固定\n","np.random.seed(seed)     # NumPyのシードを固定\n","random.seed(seed)        # Pythonのrandomモジュールのシードを固定\n","#CUDAにおけるシードの固定\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # マルチGPUを使用している場合\n","#CuDNNの再現性確保\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","#*******定数設定*******\n","DATA_DIR1 = os.path.join(BASE_DIR, 'data1/modeling_data_val')\n","DATA_DIR2 = os.path.join(BASE_DIR, 'data2/modeling_data_val')\n","\n","#被験者\n","SUBJECTS=['subject0','subject1','subject2','subject3','subject4']\n","#クラス\n","CLASSES=['unknown']\n","PRED_CLASSES=['backside_kickturn','frontside_kickturn','pumping']\n","\n","#データ属性\n","NUM_SENSORS = 72\n","SEQ_LENGTH = 250\n","BATCH_SIZE = 32\n"]},{"cell_type":"markdown","source":["#1.データローダ定義"],"metadata":{"id":"nqUqIZ5Y1t41"}},{"cell_type":"code","source":["import os\n","import math\n","import pandas as pd\n","import numpy as np\n","import random\n","import glob\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, root, subjects, mats, mask, seq_length, is_train, transform=None, cache_file='./data_cache.pt', use_cache=True):\n","\n","        self.transform = transform\n","        self.raw_seqs = [] #マスクで取捨選択するため、読み込んだ値を保存\n","\n","        self.seqs = [] #get_itemで返す値\n","        self.seq_labels = []\n","        self.subjects = os.listdir(root)\n","        self.class_names = CLASSES\n","        self.class_names.sort()\n","        self.numof_classes = len(self.class_names)\n","        self.mask = mask\n","        self.seq_length = seq_length\n","        self.is_train = is_train\n","        self.cache_file = cache_file\n","\n","        if os.path.exists(self.cache_file) and use_cache:\n","            self.load_data_from_cache()\n","        else:\n","            for (j, y) in enumerate(subjects):\n","                for mat in mats:\n","                    for (i, x) in enumerate(self.class_names):\n","                        temp = glob.glob(os.path.join(root, y, mat, x, '*'))\n","                        temp.sort()\n","                        self.seq_labels.extend([i] * len(temp))\n","\n","                        for t in temp:\n","                            df = pd.read_csv(t, header=None)\n","                            tensor = self.preprocess(df)\n","                            self.raw_seqs.append(tensor)\n","\n","            # ロードしたデータをキャッシュファイルに保存\n","            if self.cache_file!=\"\":\n","                self.save_data_to_cache()\n","\n","        self.seqs=self.blend_data(self.raw_seqs)\n","\n","    def __getitem__(self, index):\n","        seq = self.seqs[index]\n","        if self.transform is not None:\n","            seq = self.transform(seq, is_train=self.is_train, seq_length=self.seq_length)\n","\n","        #(タイムステップ,センサー)から(センサー,タイムステップ)に変換\n","        #seq = seq.T.astype(np.float32)\n","\n","        return seq, self.seq_labels[index]\n","\n","    def __len__(self):\n","        return len(self.seqs)\n","\n","\n","    #*******マスク処理関連*******\n","    def set_mask(self, mask):\n","        #マスクをセット\n","        self.mask = mask\n","        self.blend_data(self.raw_seqs)\n","\n","    def blend_data(self, data1):\n","        #マスクに応じたデータセットマスキング処理\n","        seqs=[]\n","        for i in range(0, len(self.raw_seqs)):\n","            data1 = self.raw_seqs[i]\n","\n","            result = np.zeros_like(data1)\n","            for j in range(NUM_SENSORS):\n","                if self.mask[j] == 1:\n","                    result[j, :] = data1[j, :]\n","                else:\n","                    pass #0のまま\n","\n","            seqs.append(result)\n","\n","        return seqs\n","\n","    #*******データセットロード時の加工処理*******\n","    #※試行錯誤の結果、今回は標準化のみ\n","    def preprocess(self, df: pd.DataFrame) -> np.ndarray:\n","        mat = df.T.values\n","\n","        #動作モード：1:データ①、2:データ②、3:データ①コンペ時設定\n","        if ENV_MODE!=1:  #データ1は、別途「car→標準化」を行うので標準化をパス。コンペ時はstd→car→stdを実施\n","            mat = self.standardization(mat, axis=1)\n","\n","        return mat\n","\n","    #各種前処理用関数\n","    def standardization(self, a, axis=None, ddof=0):\n","        a_mean = a.mean(axis=axis, keepdims=True)\n","        a_std = a.std(axis=axis, keepdims=True, ddof=ddof)\n","        a_std[np.where(a_std == 0)] = 1\n","        return (a - a_mean) / a_std\n","\n","    def min_max_scaling(self, a, axis=None):\n","        min_val = np.min(a, axis=axis, keepdims=True)\n","        max_val = np.max(a, axis=axis, keepdims=True)\n","        return (a - min_val) / (max_val - min_val + 1e-8)\n","\n","    #*******ロード高速化キャッシュ関連*******\n","    def save_data_to_cache(self):\n","        #キャッシュの保存\n","        os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n","        with open(self.cache_file, 'wb') as f:\n","            pickle.dump((self.raw_seqs, self.seq_labels), f)\n","        print(f\"キャッシュ生成：{self.cache_file}\")\n","\n","    def load_data_from_cache(self):\n","        #キャッシュの読込み\n","        with open(self.cache_file, 'rb') as f:\n","            self.raw_seqs, self.seq_labels = pickle.load(f)\n","\n","#*******get_item時のデータ変換関数*******\n","def transform(array, is_train, seq_length):\n","    if is_train:\n","        #今回は何もしない　※Rocketにて、どれも効果が薄かったため\n","        ts = array[:, :seq_length].astype(np.float32)\n","        return ts\n","    else:\n","        ts = array[:, :seq_length].astype(np.float32)\n","        return ts\n"],"metadata":{"id":"tV777UPa3zt1","executionInfo":{"status":"ok","timestamp":1730641672784,"user_tz":-540,"elapsed":300,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["#2.予測実行(submit生成)"],"metadata":{"id":"QRuIvEp3CjfL"}},{"cell_type":"code","source":["import ast\n","import re\n","import torch\n","import numpy as np\n","import joblib\n","import os\n","import glob\n","from scipy.io import loadmat\n","\n","#マスクに対応するモデル群からアンサンブルで回答を求める\n","#※subject(0～4) x fold(0～2) = 15モデル。各subject、foldの3モデルでアンサンブル\n","class ModelEnsemble:\n","    def __init__(self, model_dir):\n","        #モデルのディレクトリを指定して初期化\n","        self.model_dir = model_dir\n","        self.models = self._load_models()\n","        self.label_map = PRED_CLASSES\n","\n","    def _load_models(self):\n","        #各subjectのモデルを読み込む\n","        models = {}\n","        for subject in SUBJECTS:\n","            subject_models = []\n","            for fold in range(3):  #fold0-2\n","                model_path = os.path.join(self.model_dir, f\"{subject}_{fold}.pkl\")\n","                if os.path.exists(model_path):\n","                    model = joblib.load(model_path)\n","                    subject_models.append(model)\n","            if subject_models:\n","                models[f\"{subject}\"] = subject_models\n","        return models\n","\n","    def predict(self, subject, data):\n","        #指定されたsubjectのモデルでアンサンブル予測を行い、クラスラベルを返す\n","\n","        if subject not in self.models:\n","            raise ValueError(f\"モデルファイルが見つかりません：{subject}\")\n","\n","        # データを適切な形式に変換\n","        if isinstance(data, torch.Tensor):\n","            data = data.numpy()\n","        data = data.astype(np.float32)\n","\n","        # 各モデルの予測を集める\n","        predictions = []\n","        for model in self.models[subject]:\n","            pred = model.predict(data)\n","            predictions.append(pred)\n","\n","        # 多数決で最終予測を決定\n","        predictions = np.array(predictions)\n","        ensemble_pred = np.array([np.bincount(p).argmax() for p in predictions.T])\n","\n","        # 数値から文字列ラベルに変換\n","        str_predictions = [self.label_map[p] for p in ensemble_pred]\n","\n","        return str_predictions\n","\n","#*******フィルタ適用処理*******\n","def apply_car_and_normalize(data):\n","    #Common Average Referenceと正規化を実施\n","\n","    #CAR\n","    car_data = data - np.mean(data, axis=1, keepdims=True)\n","\n","    #正規化\n","    mean = np.mean(car_data, axis=2, keepdims=True)\n","    std = np.std(car_data, axis=2, keepdims=True)\n","    normalized_data = (car_data - mean) / (std + 1e-8)  #1e-8：0除算回避\n","\n","    return normalized_data\n","\n","\n","\n","\n","def predict():\n","    #*******予測実行*******\n","    case_preds_item = [[] for _ in range(3)] #各ケースでの予測結果を蓄積する [3, 問題数]\n","    case_preds_name = [[] for _ in range(3)]\n","\n","    #動作モードによりデータ参照先変更：1:データ①、2:データ②、3:データ①コンペ時設定\n","    if ENV_MODE!=2:\n","        data_dir = DATA_DIR1\n","    else:\n","        data_dir = DATA_DIR2\n","\n","\n","    #モデルディレクトリのパスを取得\n","    #指定フォルダ直下のフォルダ一覧を取得\n","    search_dir = os.path.join(BASE_DIR, f\"models_{MODEL_NAME}\", \"mask_models\")\n","    case_dirs = [f for f in os.listdir(search_dir) if os.path.isdir(os.path.join(search_dir, f))]\n","    #フォルダ名からスコアを抽出し、降順にソート\n","    case_dirs = sorted(case_dirs, key=lambda x: float(re.search(r'score([0-9.]+)_', x).group(1)), reverse=True)\n","\n","    #結果出力用フォルダ設定\n","    output_dir = os.path.join(BASE_DIR, f\"models_{MODEL_NAME}\", \"mask_models_predictions\")\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","\n","    #上位3モデルでの予測\n","    for case_idx, case_dir in enumerate(case_dirs):\n","        model_dir = os.path.join(BASE_DIR, f\"models_{MODEL_NAME}\", \"mask_models\", case_dir)\n","\n","        # モデルアンサンブルの初期化\n","        ensemble = ModelEnsemble(model_dir)\n","\n","        # ファイルを読み込む\n","        with open(os.path.join(model_dir,\"mask.txt\"), \"r\") as f:\n","            mask_data = f.read()\n","\n","        # 文字列をリストに変換\n","        mask = ast.literal_eval(mask_data)\n","\n","        #各ケースの結果出力\n","        f = open(os.path.join(output_dir,f\"submit_{case_dir}.csv\"), \"w\")\n","\n","        # 予測\n","        for subject in SUBJECTS:\n","            #データのロード\n","            val_dataset = SeqDataset(data_dir, [subject], [f'{subject}.mat'], mask=mask,\n","                                      seq_length=SEQ_LENGTH, is_train=False, transform=transform,\n","                                      cache_file='', use_cache=False)\n","\n","            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","            X_val, y_val = [], []\n","            for batch in val_loader:\n","                X, y = batch\n","                X_val.extend(X.numpy())\n","                y_val.extend(y.numpy())\n","\n","            # CAR と正規化を適用\n","            if ENV_MODE!=2:\n","                #動作モードにより実施有無変更：1:データ①、2:データ②、3:データ①コンペ時設定\n","                #※data2はクレンジング済みなので、CARは逆効果\n","                X_val = apply_car_and_normalize(X_val)\n","\n","            y_pred = ensemble.predict(subject, np.array(X_val))\n","\n","            # リストの各項目をファイルに出力\n","            for i,item in enumerate(y_pred):\n","                idx=f\"000{i}\"[-3:]\n","                line=f\"{subject}_{idx},{item}\"\n","                f.write(f\"{line}\\n\")\n","\n","                case_preds_name[case_idx].append(f\"{subject}_{idx}\")\n","                case_preds_item[case_idx].append(item)\n","\n","            print(y_pred)\n","\n","        # ファイルを閉じる\n","        f.close()\n","\n","\n","    #*******SUBMITの出力*******\n","    #結果出力用\n","    f = open(os.path.join(output_dir,f\"submit.csv\"), \"w\")\n","\n","    #多数決でsubmitを生成する\n","    for y in range(len(case_preds_name[0])): #問題数分、ループ\n","        a=case_preds_item[0][y]\n","        b=case_preds_item[1][y]\n","        c=case_preds_item[2][y]\n","        if a==b or a==c:\n","            item = a\n","        elif b==c:\n","            item = b\n","        else:\n","            item = a\n","\n","        line=f\"{case_preds_name[0][y]},{item}\"\n","        f.write(f\"{line}\\n\")\n","\n","    # ファイルを閉じる\n","    f.close()\n","\n","predict()"],"metadata":{"id":"Gen2ofTjCnPw"},"execution_count":null,"outputs":[]}]}