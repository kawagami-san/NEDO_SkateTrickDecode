{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMfKOyZvlcvlnyrXz2atxB1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#0.初期設定"],"metadata":{"id":"J4o6_noBS8F9"}},{"cell_type":"code","source":["#*******今回の実行の設定*******\n","\n","#プロジェクトのフォルダ　※BASE_DIRを、自らの環境に合わせて修正してください\n","BASE_DIR = '/content/drive/MyDrive/project01'\n","\n","#モデル名\n","MODEL_NAME = \"data1_trial1\"\n","\n","#動作モード：1:データ①、2:データ②、3:データ①コンペ時設定\n","ENV_MODE = 1\n","\n","#遺伝的アルゴリズムの集団数、実行世代数\n","NUM_POPULATIONS = 30\n","NUM_GENERATIONS = 50\n","\n","#乱数シード\n","RANDOM_SEED = 42\n","\n"],"metadata":{"id":"ZoLw6uJPqYjg","executionInfo":{"status":"ok","timestamp":1730636861659,"user_tz":-540,"elapsed":272,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0QWb5IfSkLu","executionInfo":{"status":"ok","timestamp":1730636304102,"user_tz":-540,"elapsed":7669,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}},"outputId":"11709c11-1f36-4f12-8c9f-6286e52b0fc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: sktime in /usr/local/lib/python3.10/dist-packages (0.34.0)\n","Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.4.2)\n","Requirement already satisfied: numpy<2.2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sktime) (24.1)\n","Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (2.2.2)\n","Requirement already satisfied: scikit-base<0.12.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sktime) (0.11.0)\n","Requirement already satisfied: scikit-learn<1.6.0,>=0.24 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.5.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from sktime) (1.13.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2024.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.6.0,>=0.24->sktime) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.16.0)\n"]}],"source":["#*******Googleドライブへの接続*******\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#*******不足ライブラリのインストール*******\n","!pip install sktime\n","\n","#*******ライブラリのインポート*******\n","import sys\n","import os\n","import random\n","import pandas as pd\n","import numpy as np\n","import glob\n","import torch\n","from scipy.io import loadmat\n","\n","#*******乱数シードを固定*******\n","seed = RANDOM_SEED\n","torch.manual_seed(seed)  # PyTorchのシードを固定\n","np.random.seed(seed)     # NumPyのシードを固定\n","random.seed(seed)        # Pythonのrandomモジュールのシードを固定\n","#CUDAにおけるシードの固定\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # マルチGPUを使用している場合\n","#CuDNNの再現性確保\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","#*******定数設定*******\n","DATA_DIR1 = os.path.join(BASE_DIR, 'data1/modeling_data')\n","DATA_DIR2 = os.path.join(BASE_DIR, 'data2/modeling_data')\n","\n","#被験者\n","SUBJECTS=['subject0','subject1','subject2','subject3','subject4']\n","#試行\n","TRAIN_MATS=['train1.mat','train2.mat','train3.mat']\n","#クラス\n","CLASSES=['backside_kickturn','frontside_kickturn','pumping']\n","#データ属性\n","NUM_SENSORS = 72\n","SEQ_LENGTH = 250\n","BATCH_SIZE = 32\n"]},{"cell_type":"markdown","source":["#1.データローダ定義"],"metadata":{"id":"nqUqIZ5Y1t41"}},{"cell_type":"code","source":["import os\n","import math\n","import pandas as pd\n","import numpy as np\n","import random\n","import glob\n","import pickle\n","from torch.utils.data import Dataset, DataLoader\n","\n","class SeqDataset(Dataset):\n","    def __init__(self, root, subjects, mats, mask, seq_length, is_train, transform=None, cache_file='./data_cache.pt', use_cache=True):\n","\n","        self.transform = transform\n","        self.raw_seqs = [] #マスクで取捨選択するため、読み込んだ値を保存\n","\n","        self.seqs = [] #get_itemで返す値\n","        self.seq_labels = []\n","        self.subjects = os.listdir(root)\n","        self.class_names = CLASSES\n","        self.class_names.sort()\n","        self.numof_classes = len(self.class_names)\n","        self.mask = mask\n","        self.seq_length = seq_length\n","        self.is_train = is_train\n","        self.cache_file = cache_file\n","\n","        if os.path.exists(self.cache_file) and use_cache:\n","            self.load_data_from_cache()\n","        else:\n","            for (j, y) in enumerate(subjects):\n","                for mat in mats:\n","                    for (i, x) in enumerate(self.class_names):\n","                        temp = glob.glob(os.path.join(root, y, mat, x, '*'))\n","                        temp.sort()\n","                        self.seq_labels.extend([i] * len(temp))\n","\n","                        for t in temp:\n","                            df = pd.read_csv(t, header=None)\n","                            tensor = self.preprocess(df)\n","                            self.raw_seqs.append(tensor)\n","\n","            # ロードしたデータをキャッシュファイルに保存\n","            if self.cache_file!=\"\":\n","                self.save_data_to_cache()\n","\n","        self.seqs=self.blend_data(self.raw_seqs)\n","\n","    def __getitem__(self, index):\n","        seq = self.seqs[index]\n","        if self.transform is not None:\n","            seq = self.transform(seq, is_train=self.is_train, seq_length=self.seq_length)\n","\n","        #(タイムステップ,センサー)から(センサー,タイムステップ)に変換\n","        #seq = seq.T.astype(np.float32)\n","\n","        return seq, self.seq_labels[index]\n","\n","    def __len__(self):\n","        return len(self.seqs)\n","\n","\n","    #*******マスク処理関連*******\n","    def set_mask(self, mask):\n","        #マスクをセット\n","        self.mask = mask\n","        self.blend_data(self.raw_seqs)\n","\n","    def blend_data(self, data1):\n","        #マスクに応じたデータセットマスキング処理\n","        seqs=[]\n","        for i in range(0, len(self.raw_seqs)):\n","            data1 = self.raw_seqs[i]\n","\n","            result = np.zeros_like(data1)\n","            for j in range(NUM_SENSORS):\n","                if self.mask[j] == 1:\n","                    result[j, :] = data1[j, :]\n","                else:\n","                    pass #0のまま\n","\n","            seqs.append(result)\n","\n","        return seqs\n","\n","    #*******データセットロード時の加工処理*******\n","    #※試行錯誤の結果、今回は標準化のみ\n","    def preprocess(self, df: pd.DataFrame) -> np.ndarray:\n","        mat = df.T.values\n","\n","        #動作モード：1:データ①、2:データ②、3:データ①コンペ時設定\n","        if ENV_MODE!=1:  #データ1は、別途「car→標準化」を行うので標準化をパス。コンペ時はstd→car→stdを実施\n","            mat = self.standardization(mat, axis=1)\n","\n","        return mat\n","\n","    #各種前処理用関数\n","    def standardization(self, a, axis=None, ddof=0):\n","        a_mean = a.mean(axis=axis, keepdims=True)\n","        a_std = a.std(axis=axis, keepdims=True, ddof=ddof)\n","        a_std[np.where(a_std == 0)] = 1\n","        return (a - a_mean) / a_std\n","\n","    def min_max_scaling(self, a, axis=None):\n","        min_val = np.min(a, axis=axis, keepdims=True)\n","        max_val = np.max(a, axis=axis, keepdims=True)\n","        return (a - min_val) / (max_val - min_val + 1e-8)\n","\n","    #*******ロード高速化キャッシュ関連*******\n","    def save_data_to_cache(self):\n","        #キャッシュの保存\n","        os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)\n","        with open(self.cache_file, 'wb') as f:\n","            pickle.dump((self.raw_seqs, self.seq_labels), f)\n","        print(f\"キャッシュ生成：{self.cache_file}\")\n","\n","    def load_data_from_cache(self):\n","        #キャッシュの読込み\n","        with open(self.cache_file, 'rb') as f:\n","            self.raw_seqs, self.seq_labels = pickle.load(f)\n","\n","#*******get_item時のデータ変換関数*******\n","def transform(array, is_train, seq_length):\n","    if is_train:\n","        #今回は何もしない　※Rocketにて、どれも効果が薄かったため\n","        ts = array[:, :seq_length].astype(np.float32)\n","        return ts\n","    else:\n","        ts = array[:, :seq_length].astype(np.float32)\n","        return ts\n"],"metadata":{"id":"tV777UPa3zt1","executionInfo":{"status":"ok","timestamp":1730628922918,"user_tz":-540,"elapsed":281,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["#*******動作確認*******\n","#mask=[0]*72\n","mask=[1]*72\n","\n","fold=2 #ベースラインは2とする\n","subject=SUBJECTS[1]\n","train_mats=TRAIN_MATS.copy()\n","val_mats=[train_mats.pop(fold)]\n","\n","\n","train_dataset = SeqDataset(DATA_DIR1, [subject], train_mats, mask=mask,\n","                           seq_length=SEQ_LENGTH, is_train=True, transform=transform,\n","                           cache_file=os.path.join(DATA_DIR1, 'pkl', f'ds_train_{subject}_{fold}.pt'),\n","                           use_cache=True)\n","val_dataset = SeqDataset(DATA_DIR1, [subject], val_mats, mask=mask,\n","                           seq_length=SEQ_LENGTH, is_train=True, transform=transform,\n","                           cache_file=os.path.join(DATA_DIR1, 'pkl', f'ds_val_{subject}_{fold}.pt'),\n","                           use_cache=True)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","one_batch = next(iter(train_loader))\n","\n","# ミニバッチの内容を確認\n","#print(one_batch)\n"],"metadata":{"id":"6pIZO7tf1wWz","executionInfo":{"status":"ok","timestamp":1730628926620,"user_tz":-540,"elapsed":540,"user":{"displayName":"先進WG01","userId":"08461916073753441850"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["#2.学習実行"],"metadata":{"id":"EucPNtE9K9ct"}},{"cell_type":"code","source":["import os\n","import random\n","import time\n","import joblib\n","import glob\n","import json\n","from datetime import datetime\n","from sklearn.linear_model import RidgeClassifierCV\n","from sklearn.metrics import accuracy_score\n","from sktime.transformations.panel.rocket import MiniRocketMultivariate\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","#*******上位モデルの保存*******\n","def save_mask_models(new_models, model_dir, mask, generation):\n","    #マスクモデルの保存管理\n","\n","    mask_dir = os.path.join(model_dir, \"mask_models\")\n","    os.makedirs(mask_dir, exist_ok=True)\n","\n","    # 新しいマスクの平均スコアを計算\n","    current_score = np.mean([score for _, _, score, _ in new_models])\n","\n","    # 新しいマスクのディレクトリ名\n","    new_dir_name = f\"score{current_score:.4f}_gen{generation+1}\"\n","    new_dir_path = os.path.join(mask_dir, new_dir_name)\n","\n","    # 既存のマスクディレクトリを取得してスコアとジェネレーション情報を抽出\n","    existing_dirs = []\n","    for dir_path in glob.glob(os.path.join(mask_dir, \"score*_gen*\")):\n","        dir_name = os.path.basename(dir_path)\n","        score = float(dir_name.split('_')[0].replace('score', ''))\n","        gen = int(dir_name.split('_')[1].replace('gen', ''))\n","        existing_dirs.append({\n","            'path': dir_path,\n","            'score': score,\n","            'generation': gen\n","        })\n","\n","    # スコアで降順ソート（同点の場合は世代が新しい方を優先）\n","    existing_dirs.sort(key=lambda x: (x['score'], x['generation']), reverse=True)\n","\n","    # 新しいスコアが上位3位以内に入るか確認\n","    should_save = False\n","    if len(existing_dirs) < 3:\n","        should_save = True\n","    else:\n","        min_top3_score = existing_dirs[2]['score']\n","        if current_score >= min_top3_score:\n","            should_save = True\n","\n","    if should_save:\n","        # 新しいディレクトリを作成して全モデルを保存\n","        os.makedirs(new_dir_path, exist_ok=True)\n","        for subject, fold, _, pipeline in new_models:\n","            file_name = f\"{subject}_{fold}.pkl\"\n","            file_path = os.path.join(new_dir_path, file_name)\n","            joblib.dump(pipeline, file_path)\n","\n","        # マスクを保存する\n","        with open(os.path.join(new_dir_path, 'mask.txt'), 'w', encoding='utf-8') as file:\n","            file.write(f'{mask}')\n","\n","        # 既存のディレクトリリストに新しいディレクトリを追加\n","        existing_dirs.append({\n","            'path': new_dir_path,\n","            'score': current_score,\n","            'generation': generation\n","        })\n","\n","        # 再度ソート\n","        existing_dirs.sort(key=lambda x: (x['score'], x['generation']), reverse=True)\n","\n","        # 上位3つ以外のディレクトリを削除\n","        for dir_info in existing_dirs[3:]:\n","            import shutil\n","            try:\n","                shutil.rmtree(dir_info['path'])\n","            except Exception as e:\n","                print(f\"モデル移動処理失敗：{dir_info['path']}: {str(e)}\")\n","\n","def update_model_rankings(new_models, model_dir, mask, generation):\n","    #モデルの保存を管理する関数\n","\n","    try:\n","        # マスクモデルの保存\n","        save_mask_models(new_models, model_dir, mask, generation)\n","\n","        return True\n","    except Exception as e:\n","        print(f\"モデル更新エラー:{str(e)}\")\n","        return False\n","\n","\n","#*******フィルタ適用処理*******\n","def apply_car_and_normalize(data):\n","    #Common Average Referenceと正規化を実施\n","\n","    #CAR\n","    car_data = data - np.mean(data, axis=1, keepdims=True)\n","\n","    #正規化\n","    mean = np.mean(car_data, axis=2, keepdims=True)\n","    std = np.std(car_data, axis=2, keepdims=True)\n","    normalized_data = (car_data - mean) / (std + 1e-8)  #1e-8：0除算回避\n","\n","    return normalized_data\n","\n","def prepare_data(train_loader, val_loader):\n","    #機械学習用データ作成\n","    #※DLから移行した名残り。ローダから全データを取り込んでテーブル化する\n","\n","    X_train, y_train = [], []\n","    X_val, y_val = [], []\n","\n","    for batch in train_loader:\n","        X, y = batch\n","        X_train.extend(X.numpy())\n","        y_train.extend(y.numpy())\n","\n","    for batch in val_loader:\n","        X, y = batch\n","        X_val.extend(X.numpy())\n","        y_val.extend(y.numpy())\n","\n","    X_train = np.array(X_train)\n","    y_train = np.array(y_train)\n","    X_val = np.array(X_val)\n","    y_val = np.array(y_val)\n","\n","    # CAR と正規化を適用\n","    if ENV_MODE!=2:\n","        #動作モードにより実施有無変更：1:データ①、2:データ②、3:データ①コンペ時設定\n","        #※data2はクレンジング済みなので、CARは逆効果\n","        X_train = apply_car_and_normalize(X_train)\n","        X_val = apply_car_and_normalize(X_val)\n","\n","    return X_train, y_train, X_val, y_val\n","\n","#*******モデル学習・評価*******\n","# ROCKETモデルの学習関数\n","def train_rocket_model(X_train, y_train):\n","    rocket = MiniRocketMultivariate(num_kernels=10000, random_state=RANDOM_SEED)\n","    scaler = StandardScaler(with_mean=True)\n","    classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10))\n","    pipeline = make_pipeline(rocket, scaler, classifier)\n","    pipeline.fit(X_train, y_train)\n","    return pipeline\n","\n","def evaluate_model(model_name, mask, generation):\n","\n","    #動作モードによりデータ参照先変更：1:データ①、2:データ②、3:データ①コンペ時設定\n","    if ENV_MODE!=2:\n","        data_dir = DATA_DIR1\n","    else:\n","        data_dir = DATA_DIR2\n","\n","    #保存先作成\n","    model_dir = os.path.join(BASE_DIR, f\"models_{model_name}\")\n","    os.makedirs(model_dir, exist_ok=True)\n","\n","    new_models = []\n","    total_scores = []\n","\n","    for subject in SUBJECTS:\n","        subject_scores = []\n","        for fold in range(3):\n","\n","            train_mats=TRAIN_MATS.copy()\n","            val_mats=[train_mats.pop(fold)]\n","\n","            train_dataset = SeqDataset(data_dir, [subject], train_mats, mask=mask,\n","                                      seq_length=SEQ_LENGTH, is_train=True, transform=transform,\n","                                      cache_file=os.path.join(data_dir, 'pkl', f'ds_train_{subject}_{fold}.pt'),\n","                                      use_cache=True)\n","            val_dataset = SeqDataset(data_dir, [subject], val_mats, mask=mask,\n","                                      seq_length=SEQ_LENGTH, is_train=False, transform=transform,\n","                                      cache_file=os.path.join(data_dir, 'pkl', f'ds_valid_{subject}_{fold}.pt'),\n","                                      use_cache=True)\n","\n","            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","            X_train, y_train, X_val, y_val = prepare_data(train_loader, val_loader)\n","            pipeline = train_rocket_model(X_train, y_train)\n","\n","            y_pred = pipeline.predict(X_val)\n","            accuracy = accuracy_score(y_val, y_pred)\n","\n","            subject_scores.append(accuracy)\n","            new_models.append((subject, fold, accuracy, pipeline))\n","\n","        total_scores.append(np.mean(subject_scores))\n","\n","    # generationパラメータを追加して更新関数を呼び出し\n","    update_model_rankings(new_models, model_dir, mask, generation)\n","\n","    avg_score = np.mean(total_scores)\n","    return avg_score\n","\n","\n","#*******遺伝的アルゴリズム関連*******\n","def initialize_population(population_size, mask_size):\n","    #初期の集団を生成する\n","    population = []\n","    # ランダムなマスク\n","    for _ in range(population_size - 2):\n","        mask = [random.choice([0, 1]) for _ in range(mask_size)]\n","        population.append(mask)\n","    # すべて0のマスク\n","    population.append([0] * mask_size)\n","    # すべて1のマスク\n","    population.append([1] * mask_size)\n","    return population\n","\n","def crossover(parent1, parent2):\n","    #交差処理\n","    crossover_point = random.randint(1, len(parent1) - 1)\n","    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n","    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n","    return child1, child2\n","\n","def mutate(mask, mutation_rate):\n","    #突然変異処理\n","    return [1 - bit if random.random() < mutation_rate else bit for bit in mask]\n","\n","def adjust_mask(mask):\n","    #不使用：マスクの1が一定数を超えないようにする\n","    #※必要最低限のセンサーを探索するために使用した。将来用に残す\n","\n","    SENSORS_LIMIT=999 #不使用のため、72以上にして、ヒットしないようにする\n","\n","    ones_count = sum(mask)\n","    if ones_count > SENSORS_LIMIT:\n","        indices = [i for i, bit in enumerate(mask) if bit == 1]\n","        to_zero = random.sample(indices, ones_count - SENSORS_LIMIT)\n","        for i in to_zero:\n","            mask[i] = 0\n","    return mask\n","\n","def genetic_algorithm(model_name, population_size=20, generations=30, mutation_rate=0.01, elite_size=2, selection_rate=0.3):\n","    mask_size = NUM_SENSORS\n","    log_file_path = os.path.join(BASE_DIR, f\"ga_evaluation_log_{model_name}.txt\")\n","\n","    # 最後のジェネレーション番号を取得\n","    last_generation = get_last_generation(log_file_path)\n","    if last_generation == 0:\n","        population = initialize_population(population_size, mask_size)\n","    else:\n","        population = load_population(log_file_path, last_generation, population_size)\n","\n","    start_time = time.time()\n","    best_overall_mask = None\n","    best_overall_score = float('-inf')\n","\n","    for generation in range(last_generation, generations):\n","        fitness_scores = []\n","        for mask in population:\n","            adjusted_mask = adjust_mask(mask)\n","            avg_score = evaluate_model(model_name, adjusted_mask, generation)\n","            fitness_scores.append(avg_score)\n","\n","            if avg_score > best_overall_score:\n","                best_overall_score = avg_score\n","                best_overall_mask = adjusted_mask\n","\n","        # 適応度でソート\n","        sorted_population = sorted(zip(population, fitness_scores), key=lambda x: x[1], reverse=True)\n","\n","        # ログ出力部分（変更なし）\n","        top_3 = sorted_population[:3]\n","        worst = sorted_population[-2:]\n","        elapsed_time = time.time() - start_time\n","        print(f\"Generation {generation + 1}, Time: {elapsed_time:.2f}s\")\n","        for i, (mask, score) in enumerate(top_3):\n","            print(f\"  Top {i + 1}: Score = {score:.4f}, Mask = {adjust_mask(mask)}\")\n","        for i, (mask, score) in enumerate(worst):\n","            print(f\"  Worst {i + 1}: Score = {score:.4f}, Mask = {adjust_mask(mask)}\")\n","\n","        with open(log_file_path, \"a\") as log_file:\n","            for i, (mask, score) in enumerate(sorted_population):\n","                log_file.write(f\"Generation {generation + 1}, Rank {i + 1}, Score = {score:.4f}, Mask = {adjust_mask(mask)}\\n\")\n","\n","        # 新しい世代の生成\n","        new_population = []\n","\n","        # エリート選択（上位2個体）\n","        new_population.extend([mask for mask, _ in sorted_population[:elite_size]])\n","\n","        # 選択プールの作成（上位N%の個体）\n","        selection_pool_size = int(population_size * selection_rate)\n","        selection_pool = [mask for mask, _ in sorted_population[:selection_pool_size]]\n","        selection_pool_scores = [score for _, score in sorted_population[:selection_pool_size]]\n","\n","        # 残りの個体を生成\n","        while len(new_population) < population_size:\n","            # selection_pool(上位N%)からルーレット選択で親を選択\n","            parent1 = random.choices(selection_pool, weights=selection_pool_scores)[0]\n","            parent2 = random.choices(selection_pool, weights=selection_pool_scores)[0]\n","\n","            # 交差と突然変異\n","            child1, child2 = crossover(parent1, parent2)\n","            child1 = mutate(child1, mutation_rate)\n","            child2 = mutate(child2, mutation_rate)\n","\n","            new_population.extend([child1, child2])\n","\n","        # population_sizeに合わせる\n","        population = new_population[:population_size]\n","\n","    return best_overall_mask\n","\n","def get_last_generation(log_file_path):\n","    if not os.path.exists(log_file_path):\n","        print(f\"実行ログがありません。初期状態から開始しします\")\n","        return 0\n","\n","\n","    with open(log_file_path, 'r') as f:\n","        lines = f.readlines()\n","        if not lines:\n","            return 0\n","        last_line = lines[-1]\n","        generation = int(last_line.split(',')[0].split()[-1])\n","        print(f\"実行ログが見つかりました。ここから継続実行します。世代：{generation}\")\n","        return generation\n","\n","def load_population(log_file_path, generation, population_size):\n","    population = []\n","    with open(log_file_path, 'r') as f:\n","        lines = f.readlines()\n","        for line in reversed(lines):\n","            if line.startswith(f\"Generation {generation}\"):\n","                mask_str = line.split('Mask = ')[-1].strip()\n","                mask = eval(mask_str)  # 文字列をリストに変換\n","                population.append(mask)\n","                if len(population) == population_size:\n","                    break\n","    return population\n","\n","def run_genetic_algorithm():\n","\n","    #世代更新\n","    best_mask = genetic_algorithm(MODEL_NAME, population_size=NUM_POPULATIONS, generations=NUM_GENERATIONS)\n","\n","    #最終評価\n","    #avg_score = evaluate_model(MODEL_NAME, best_mask, NUM_GENERATIONS)\n","    print(f\"Final Result:\")\n","    #print(f\"Model: {MODEL_NAME}\")\n","    #print(f\"Best Score: {avg_score:.4f}\")\n","    print(f\"Best Mask: {best_mask}\")\n","\n","\n","#*******遺伝的アルゴリズムの実行*******\n","run_genetic_algorithm()\n"],"metadata":{"id":"VDxHb_UAlqnW"},"execution_count":null,"outputs":[]}]}